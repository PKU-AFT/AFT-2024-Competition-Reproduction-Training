{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook contains a pipeline for a **PyTorch TCN** to **predict stock movements** for the **Optiver Trading Challenge**.\n",
    "- Customizable Neural Network\n",
    "- Preprocessing and Normalization of Input Data\n",
    "- Feature Engineering\n",
    "- Decaying Learning Rate and Early Stopping\n",
    "- Hardware Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvements\n",
    "- [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) for Bottlenecks\n",
    "- Hyperparameter Tuning with [RayTune](https://docs.ray.io/en/latest/tune/index.html) (lr, layers, batchsize)\n",
    "- Flag filled NaN for near_price and far_price\n",
    "- Regularisation methods for deeper networks (batchnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:26.884395Z",
     "iopub.status.busy": "2024-12-19T11:31:26.884033Z",
     "iopub.status.idle": "2024-12-19T11:31:27.212070Z",
     "shell.execute_reply": "2024-12-19T11:31:27.211196Z",
     "shell.execute_reply.started": "2024-12-19T11:31:26.884366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "plt.tight_layout()\n",
    "plt.rcParams['figure.figsize'] = (6, 3)\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:27.213886Z",
     "iopub.status.busy": "2024-12-19T11:31:27.213517Z",
     "iopub.status.idle": "2024-12-19T11:31:31.774159Z",
     "shell.execute_reply": "2024-12-19T11:31:31.773429Z",
     "shell.execute_reply.started": "2024-12-19T11:31:27.213856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:31.776047Z",
     "iopub.status.busy": "2024-12-19T11:31:31.775253Z",
     "iopub.status.idle": "2024-12-19T11:31:31.814269Z",
     "shell.execute_reply": "2024-12-19T11:31:31.813370Z",
     "shell.execute_reply.started": "2024-12-19T11:31:31.776009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset&Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:31.924272Z",
     "iopub.status.busy": "2024-12-19T11:31:31.923985Z",
     "iopub.status.idle": "2024-12-19T11:31:31.937494Z",
     "shell.execute_reply": "2024-12-19T11:31:31.936740Z",
     "shell.execute_reply.started": "2024-12-19T11:31:31.924243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Union, List\n",
    "from copy import deepcopy\n",
    "import bisect\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:31.938735Z",
     "iopub.status.busy": "2024-12-19T11:31:31.938490Z",
     "iopub.status.idle": "2024-12-19T11:31:31.947657Z",
     "shell.execute_reply": "2024-12-19T11:31:31.946954Z",
     "shell.execute_reply.started": "2024-12-19T11:31:31.938716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lazy_sort_index(df: pd.DataFrame, axis=0) -> pd.DataFrame:\n",
    "    idx = df.index if axis == 0 else df.columns\n",
    "    if (\n",
    "        not idx.is_monotonic_increasing\n",
    "        and isinstance(idx, pd.MultiIndex)\n",
    "        and not idx.is_lexsorted()\n",
    "    ):  \n",
    "        return df.sort_index(axis=axis)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def np_ffill(arr: np.array):\n",
    "    mask = np.isnan(arr.astype(float))  # np.isnan only works on np.float\n",
    "    # get fill index\n",
    "    idx = np.where(~mask, np.arange(mask.shape[0]), 0)\n",
    "    np.maximum.accumulate(idx, out=idx)\n",
    "    return arr[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:31.948954Z",
     "iopub.status.busy": "2024-12-19T11:31:31.948700Z",
     "iopub.status.idle": "2024-12-19T11:31:31.973962Z",
     "shell.execute_reply": "2024-12-19T11:31:31.973072Z",
     "shell.execute_reply.started": "2024-12-19T11:31:31.948933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TSDataSampler:\n",
    "    \"\"\"\n",
    "    (T)ime-(S)eries DataSampler\n",
    "    This is the result of TSDatasetH\n",
    "\n",
    "    It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series\n",
    "    dataset based on tabular data.\n",
    "    - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future\n",
    "      data.\n",
    "\n",
    "    If user have further requirements for processing data, user could process them based on `TSDataSampler` or create\n",
    "    more powerful subclasses.\n",
    "\n",
    "    Known Issues:\n",
    "    - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result\n",
    "      in a different data type\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, data: pd.DataFrame, start, end, step_len: int, fillna_type: str = \"none\", dtype=None, flt_data=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a dataset which looks like torch.data.utils.Dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            The raw tabular data\n",
    "        start :\n",
    "            The indexable start time\n",
    "        end :\n",
    "            The indexable end time\n",
    "        step_len : int\n",
    "            The length of the time-series step\n",
    "        fillna_type : int\n",
    "            How will qlib handle the sample if there is on sample in a specific date.\n",
    "            none:\n",
    "                fill with np.nan\n",
    "            ffill:\n",
    "                ffill with previous sample\n",
    "            ffill+bfill:\n",
    "                ffill with previous samples first and fill with later samples second\n",
    "        flt_data : pd.Series\n",
    "            a column of data(True or False) to filter data.\n",
    "            None:\n",
    "                kepp all data\n",
    "\n",
    "        \"\"\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.step_len = step_len\n",
    "        self.fillna_type = fillna_type\n",
    "        self.data = lazy_sort_index(data)\n",
    "\n",
    "        kwargs = {\"object\": self.data}\n",
    "        if dtype is not None:\n",
    "            kwargs[\"dtype\"] = dtype\n",
    "\n",
    "        self.data_arr = np.array(**kwargs)  # Get index from numpy.array will much faster than DataFrame.values!\n",
    "        # NOTE:\n",
    "        # - append last line with full NaN for better performance in `__getitem__`\n",
    "        # - Keep the same dtype will result in a better performance\n",
    "        self.data_arr = np.append(\n",
    "            self.data_arr, np.full((1, self.data_arr.shape[1]), np.nan, dtype=self.data_arr.dtype), axis=0\n",
    "        )\n",
    "        self.nan_idx = -1  # The last line is all NaN\n",
    "\n",
    "        # the data type will be changed\n",
    "        # The index of usable data is between start_idx and end_idx\n",
    "        self.idx_df, self.idx_map = self.build_index(self.data)\n",
    "        self.data_index = deepcopy(self.data.index)\n",
    "\n",
    "        if flt_data is not None:\n",
    "            if isinstance(flt_data, pd.DataFrame):\n",
    "                assert len(flt_data.columns) == 1\n",
    "                flt_data = flt_data.iloc[:, 0]\n",
    "            # NOTE: bool(np.nan) is True !!!!!!!!\n",
    "            # make sure reindex comes first. Otherwise extra NaN may appear.\n",
    "            flt_data = flt_data.reindex(self.data_index).fillna(False).astype(np.bool)\n",
    "            self.flt_data = flt_data.values\n",
    "            self.idx_map = self.flt_idx_map(self.flt_data, self.idx_map)\n",
    "            self.data_index = self.data_index[np.where(self.flt_data)[0]]\n",
    "        self.idx_map = self.idx_map2arr(self.idx_map)\n",
    "\n",
    "        self.start_idx, self.end_idx = self.data_index.slice_locs(\n",
    "            start=start, end=end)\n",
    "        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)  # for better performance\n",
    "\n",
    "        del self.data  # save memory\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_map2arr(idx_map):\n",
    "        # pytorch data sampler will have better memory control without large dict or list\n",
    "        # - https://github.com/pytorch/pytorch/issues/13243\n",
    "        # - https://github.com/airctic/icevision/issues/613\n",
    "        # So we convert the dict into int array.\n",
    "        # The arr_map is expected to behave the same as idx_map\n",
    "\n",
    "        dtype = np.int64\n",
    "        # set a index out of bound to indicate the none existing\n",
    "        no_existing_idx = (np.iinfo(dtype).max, np.iinfo(dtype).max)\n",
    "\n",
    "        max_idx = max(idx_map.keys())\n",
    "        arr_map = []\n",
    "        for i in range(max_idx + 1):\n",
    "            arr_map.append(idx_map.get(i, no_existing_idx))\n",
    "        arr_map = np.array(arr_map, dtype=dtype)\n",
    "        return arr_map\n",
    "\n",
    "    @staticmethod\n",
    "    def flt_idx_map(flt_data, idx_map):\n",
    "        idx = 0\n",
    "        new_idx_map = {}\n",
    "        for i, exist in enumerate(flt_data):\n",
    "            if exist:\n",
    "                new_idx_map[idx] = idx_map[i]\n",
    "                idx += 1\n",
    "        return new_idx_map\n",
    "\n",
    "    def get_index(self):\n",
    "        \"\"\"\n",
    "        Get the pandas index of the data, it will be useful in following scenarios\n",
    "        - Special sampler will be used (e.g. user want to sample day by day)\n",
    "        \"\"\"\n",
    "        return self.data_index[self.start_idx : self.end_idx]\n",
    "\n",
    "    def config(self, **kwargs):\n",
    "        # Config the attributes\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_index(data: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "        \"\"\"\n",
    "        The relation of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            The dataframe with <datetime, DataFrame>\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[pd.DataFrame, dict]:\n",
    "            1) the first element:  reshape the original index into a <datetime(row), instrument(column)> 2D dataframe\n",
    "                instrument SH600000 SH600004 SH600006 SH600007 SH600008 SH600009  ...\n",
    "                datetime\n",
    "                2021-01-11        0        1        2        3        4        5  ...\n",
    "                2021-01-12     4146     4147     4148     4149     4150     4151  ...\n",
    "                2021-01-13     8293     8294     8295     8296     8297     8298  ...\n",
    "                2021-01-14    12441    12442    12443    12444    12445    12446  ...\n",
    "            2) the second element:  {<original index>: <row, col>}\n",
    "        \"\"\"\n",
    "        # object incase of pandas converting int to float\n",
    "        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=object)\n",
    "        idx_df = lazy_sort_index(idx_df.unstack())\n",
    "        # NOTE: the correctness of `__getitem__` depends on columns sorted here\n",
    "        idx_df = lazy_sort_index(idx_df, axis=1)\n",
    "\n",
    "        idx_map = {}\n",
    "        for i, (_, row) in enumerate(idx_df.iterrows()):\n",
    "            for j, real_idx in enumerate(row):\n",
    "                if not np.isnan(real_idx):\n",
    "                    idx_map[real_idx] = (i, j)\n",
    "        return idx_df, idx_map\n",
    "\n",
    "    @property\n",
    "    def empty(self):\n",
    "        return len(self) == 0\n",
    "\n",
    "    def _get_indices(self, row: int, col: int) -> np.array:\n",
    "        \"\"\"\n",
    "        get series indices of self.data_arr from the row, col indices of self.idx_df\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row : int\n",
    "            the row in self.idx_df\n",
    "        col : int\n",
    "            the col in self.idx_df\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            The indices of data of the data\n",
    "        \"\"\"\n",
    "        indices = self.idx_arr[max(row - self.step_len + 1, 0) : row + 1, col]\n",
    "\n",
    "        if len(indices) < self.step_len:\n",
    "            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])\n",
    "\n",
    "        if self.fillna_type == \"ffill\":\n",
    "            indices = np_ffill(indices)\n",
    "        elif self.fillna_type == \"ffill+bfill\":\n",
    "            indices = np_ffill(np_ffill(indices)[::-1])[::-1]\n",
    "        else:\n",
    "            assert self.fillna_type == \"none\"\n",
    "        return indices\n",
    "\n",
    "    def _get_row_col(self, idx) -> Tuple[int]:\n",
    "        \"\"\"\n",
    "        get the col index and row index of a given sample index in self.idx_df\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx :\n",
    "            the input of  `__getitem__`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[int]:\n",
    "            the row and col index\n",
    "        \"\"\"\n",
    "        # The the right row number `i` and col number `j` in idx_df\n",
    "        if isinstance(idx, (int, np.integer)):\n",
    "            real_idx = self.start_idx + idx\n",
    "            if self.start_idx <= real_idx < self.end_idx:\n",
    "                i, j = self.idx_map[real_idx]  # TODO: The performance of this line is not good\n",
    "            else:\n",
    "                raise KeyError(f\"{real_idx} is out of [{self.start_idx}, {self.end_idx})\")\n",
    "        elif isinstance(idx, tuple):\n",
    "            # <TSDataSampler object>[\"datetime\", \"instruments\"]\n",
    "            date, inst = idx\n",
    "            date = pd.Timestamp(date)\n",
    "            i = bisect.bisect_right(self.idx_df.index, date) - 1\n",
    "            # NOTE: This relies on the idx_df columns sorted in `__init__`\n",
    "            j = bisect.bisect_left(self.idx_df.columns, inst)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"This type of input is not supported\")\n",
    "        return i, j\n",
    "\n",
    "    def __getitem__(self, idx: Union[int, Tuple[object, str], List[int]]):\n",
    "        \"\"\"\n",
    "        # We have two method to get the time-series of a sample\n",
    "        tsds is a instance of TSDataSampler\n",
    "\n",
    "        # 1) sample by int index directly\n",
    "        tsds[len(tsds) - 1]\n",
    "\n",
    "        # 2) sample by <datetime,instrument> index\n",
    "        tsds['2016-12-31', \"SZ300315\"]\n",
    "\n",
    "        # The return value will be similar to the data retrieved by following code\n",
    "        df.loc(axis=0)['2015-01-01':'2016-12-31', \"SZ300315\"].iloc[-30:]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : Union[int, Tuple[object, str]]\n",
    "        \"\"\"\n",
    "        # Multi-index type\n",
    "        mtit = (list, np.ndarray)\n",
    "        if isinstance(idx, mtit):\n",
    "            indices = [self._get_indices(*self._get_row_col(i)) for i in idx]\n",
    "            indices = np.concatenate(indices)\n",
    "        else:\n",
    "            indices = self._get_indices(*self._get_row_col(idx))\n",
    "\n",
    "        # 1) for better performance, use the last nan line for padding the lost date\n",
    "        # 2) In case of precision problems. We use np.float64. # TODO: I'm not sure if whether np.float64 will result in\n",
    "        # precision problems. It will not cause any problems in my tests at least\n",
    "        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)\n",
    "\n",
    "        data = self.data_arr[indices]\n",
    "        if isinstance(idx, mtit):\n",
    "            # if we get multiple indexes, addition dimension should be added.\n",
    "            # <sample_idx, step_idx, feature_idx>\n",
    "            data = data.reshape(-1, self.step_len, *data.shape[1:])\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:31.975463Z",
     "iopub.status.busy": "2024-12-19T11:31:31.975065Z",
     "iopub.status.idle": "2024-12-19T11:31:31.989139Z",
     "shell.execute_reply": "2024-12-19T11:31:31.988445Z",
     "shell.execute_reply.started": "2024-12-19T11:31:31.975400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DailyBatchSamplerRandom(Sampler):\n",
    "    def __init__(self, data_source, shuffle=False):\n",
    "        self.data_source = data_source\n",
    "        self.shuffle = shuffle\n",
    "        # calculate number of samples in each batch\n",
    "        self.daily_count = pd.Series(index=self.data_source.get_index(), dtype=pd.Float32Dtype).groupby(\"time_id\").size().values\n",
    "        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch\n",
    "        self.daily_index[0] = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            index = np.arange(len(self.daily_count))\n",
    "            np.random.shuffle(index)\n",
    "            for i in index:\n",
    "                yield np.arange(self.daily_index[i], self.daily_index[i] + self.daily_count[i])\n",
    "        else:\n",
    "            for idx, count in zip(self.daily_index, self.daily_count):\n",
    "                yield np.arange(idx, idx + count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:31:31.990662Z",
     "iopub.status.busy": "2024-12-19T11:31:31.990378Z",
     "iopub.status.idle": "2024-12-19T11:32:21.204399Z",
     "shell.execute_reply": "2024-12-19T11:32:21.203441Z",
     "shell.execute_reply.started": "2024-12-19T11:31:31.990642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/phbs-data-for-model/data_for_model.parquet', 'rb') as f:\n",
    "    data =   pd.read_parquet(f)\n",
    "\n",
    "data = data.drop(['row_id', 'seconds_in_bucket', 'date_id', '__index_level_0__'], axis=1)\n",
    "\n",
    "cols = list(data.columns)\n",
    "cols.remove('target')\n",
    "cols.append('target')\n",
    "\n",
    "data = data[cols]\n",
    "\n",
    "# to float32\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "print(data.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:32:21.205765Z",
     "iopub.status.busy": "2024-12-19T11:32:21.205495Z",
     "iopub.status.idle": "2024-12-19T11:32:24.243407Z",
     "shell.execute_reply": "2024-12-19T11:32:24.242445Z",
     "shell.execute_reply.started": "2024-12-19T11:32:21.205741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = data.set_index(['time_id', 'stock_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:32:24.244716Z",
     "iopub.status.busy": "2024-12-19T11:32:24.244459Z",
     "iopub.status.idle": "2024-12-19T11:32:30.399115Z",
     "shell.execute_reply": "2024-12-19T11:32:30.398377Z",
     "shell.execute_reply.started": "2024-12-19T11:32:24.244694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_time_id = data.index.get_level_values('time_id').max()\n",
    "\n",
    "print(\"max_time_id\", max_time_id)\n",
    "\n",
    "# split 4:1\n",
    "split_time_id = int(max_time_id * 0.8)\n",
    "\n",
    "train_data = data.query('time_id <= @split_time_id').copy()\n",
    "valid_data = data.query('time_id > @split_time_id').copy()\n",
    "\n",
    "# del data\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-19T18:25:57.226Z",
     "iopub.execute_input": "2024-12-19T11:32:30.400351Z",
     "iopub.status.busy": "2024-12-19T11:32:30.400090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = TSDataSampler(data=train_data, start=0, end=split_time_id, step_len=10, fillna_type='ffill+bfill',)\n",
    "valid_dataset = TSDataSampler(data=valid_data, start=split_time_id+1, end=max_time_id, step_len=10, fillna_type='ffill+bfill',)\n",
    "\n",
    "train_sampler = DailyBatchSamplerRandom(train_dataset, shuffle=False)\n",
    "valid_sampler = DailyBatchSamplerRandom(valid_dataset, shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-19T18:25:57.230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    data = torch.squeeze(data, dim=0)\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:01:54.462469Z",
     "iopub.status.busy": "2024-12-19T14:01:54.461669Z",
     "iopub.status.idle": "2024-12-19T14:01:54.477682Z",
     "shell.execute_reply": "2024-12-19T14:01:54.476839Z",
     "shell.execute_reply.started": "2024-12-19T14:01:54.462441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "\n",
    "class SequenceModel2():\n",
    "\n",
    "    def __init__(self, n_epochs, lr, GPU=None, seed=None, train_stop_loss_thred=None, save_path='/kaggle/working/', save_prefix=''):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.seed = seed\n",
    "        self.train_stop_loss_thred = train_stop_loss_thred\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "        self.fitted = False\n",
    "        self.model = None\n",
    "        self.train_optimizer = None\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.save_prefix = save_prefix\n",
    "\n",
    "    def load_param(self, param_path):\n",
    "        self.model.load_state_dict(torch.load(param_path, map_location=self.device))\n",
    "        self.fitted = True\n",
    "        \n",
    "    def _init_data_loader(self, data, shuffle=True, drop_last=True):\n",
    "        sampler = DailyBatchSamplerRandom(data, shuffle)\n",
    "        data_loader = DataLoader(data, sampler=sampler, drop_last=drop_last)\n",
    "        return data_loader\n",
    "\n",
    "    def init_model(self):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"model has not been initialized\")\n",
    "\n",
    "        self.train_optimizer = optim.Adam(self.model.parameters(), self.lr)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.train_optimizer, mode='min', factor=0.1, patience=10)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def loss_fn(self, pred, label):\n",
    "        mask = ~torch.isnan(label)\n",
    "        loss = torch.abs(pred[mask] - label[mask])\n",
    "        return torch.mean(loss)\n",
    "\n",
    "    # def mse_loss(self, pred, label):\n",
    "    #     return torch.mean((pred - label) ** 2)\n",
    "\n",
    "    # def dtw_loss(self, pred, label):\n",
    "    #     pred_np = pred.cpu().detach().numpy()\n",
    "    #     label_np = label.cpu().detach().numpy()\n",
    "    #     distance, _ = fastdtw(pred_np, label_np, dist=lambda x, y: np.linalg.norm(x - y, ord=1))\n",
    "    #     return torch.tensor(distance, requires_grad=True)\n",
    "\n",
    "    # def loss_fn(self, pred, label):\n",
    "    #     \"\"\"IC-based loss function.\"\"\"\n",
    "    #     mask = ~torch.isnan(label)\n",
    "    #     pred = pred[mask].detach().cpu().numpy()  # 转换为 NumPy 数组\n",
    "    #     label = label[mask].detach().cpu().numpy()  # 转换为 NumPy 数组\n",
    "    #     ic = spearmanr(pred, label).correlation\n",
    "    #     loss = 1 - ic  # Maximize IC by minimizing (1 - IC)\n",
    "        \n",
    "    #     return torch.tensor(loss, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "    def train_epoch(self, data_loader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        for data in data_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1].to(self.device)\n",
    "\n",
    "            pred = self.model(feature.float())\n",
    "            loss = self.loss_fn(pred, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.train_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.model.parameters(), 1.0)\n",
    "            self.train_optimizer.step()\n",
    "\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    def test_epoch(self, data_loader):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "\n",
    "        for data in data_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1].to(self.device)\n",
    "            pred = self.model(feature.float())\n",
    "            loss = self.loss_fn(pred, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    def fit(self, dl_train, dl_valid):\n",
    "        train_loader = self._init_data_loader(dl_train, shuffle=True, drop_last=True)\n",
    "        valid_loader = self._init_data_loader(dl_valid, shuffle=False, drop_last=True)\n",
    "\n",
    "        self.fitted = True\n",
    "        best_param = None\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        for step in range(self.n_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.test_epoch(valid_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            print(\"Epoch %d, train_loss %.6f, valid_loss %.6f \" % (step, train_loss, val_loss))\n",
    "            best_param = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            if early_stopping(val_loss):\n",
    "                break\n",
    "\n",
    "            torch.save(best_param, f'{self.save_path}{self.save_prefix}master_{step}.pkl')\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:01:58.392588Z",
     "iopub.status.busy": "2024-12-19T14:01:58.391742Z",
     "iopub.status.idle": "2024-12-19T14:01:58.406835Z",
     "shell.execute_reply": "2024-12-19T14:01:58.405941Z",
     "shell.execute_reply.started": "2024-12-19T14:01:58.392539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    用于移除卷积后序列末尾的padding\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size]\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    TCN的基本构建块，包含两层因果卷积\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                              stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                              stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                               self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的TCN模型实现\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TCNModel, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size,\n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size,\n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入x的形状: (batch_size, seq_len, input_size)\n",
    "        # 需要转换为TCN期望的形状: (batch_size, input_size, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # 应用TCN层\n",
    "        x = self.network(x)\n",
    "        \n",
    "        # 只取最后一个时间步的输出\n",
    "        x = x[:, :, -1]\n",
    "        \n",
    "        # 应用最终的线性层得到预测\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class TCNSequenceModel(SequenceModel2):\n",
    "    \"\"\"\n",
    "    继承自SequenceModel的TCN模型类\n",
    "    \"\"\"\n",
    "    def __init__(self, n_epochs, lr, input_size, output_size=1, num_channels=[32, 64, 128], \n",
    "                 kernel_size=2, dropout=0.2, **kwargs):\n",
    "        super(TCNSequenceModel, self).__init__(n_epochs, lr, **kwargs)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_channels = num_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 初始化TCN模型\n",
    "        self.model = TCNModel(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 初始化模型参数\n",
    "        self.init_model()\n",
    "\n",
    "    def fit(self, dl_train, dl_valid):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \"\"\"\n",
    "        # 检查输入特征维度是否正确\n",
    "        # sample_data = next(iter(self._init_data_loader(dl_train)))\n",
    "        # sample_data = torch.squeeze(sample_data, dim=0)\n",
    "        # assert sample_data.shape[2]-1 == self.input_size, \\\n",
    "        #     f\"Input feature dimension {sample_data.shape[2]-1} doesn't match model input size {self.input_size}\"\n",
    "        \n",
    "        # 调用父类的fit方法\n",
    "        train_losses, val_losses = super().fit(dl_train, dl_valid)\n",
    "        return train_losses, val_losses\n",
    "        ## super().fit(dl_train, dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:02:05.316137Z",
     "iopub.status.busy": "2024-12-19T14:02:05.315460Z",
     "iopub.status.idle": "2024-12-19T14:02:09.685836Z",
     "shell.execute_reply": "2024-12-19T14:02:09.684613Z",
     "shell.execute_reply.started": "2024-12-19T14:02:05.316110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化模型\n",
    "model = TCNSequenceModel(\n",
    "    n_epochs=100,\n",
    "    lr=0.001,\n",
    "    input_size=124,  # 特征维度（124-1，因为最后一维是标签）\n",
    "    output_size=1,   # 预测维度\n",
    "    num_channels=[32, 64, 128],  # TCN的通道数配置\n",
    "    kernel_size=2,   # 卷积核大小\n",
    "    dropout=0.2      # dropout率\n",
    ")\n",
    "\n",
    "# 训练模型并记录损失\n",
    "train_losses, val_losses = model.fit(train_dataset, valid_dataset)\n",
    "\n",
    "# 绘制损失变化图像\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T12:44:06.537674Z",
     "iopub.status.busy": "2024-12-19T12:44:06.537409Z",
     "iopub.status.idle": "2024-12-19T13:50:18.651683Z",
     "shell.execute_reply": "2024-12-19T13:50:18.650039Z",
     "shell.execute_reply.started": "2024-12-19T12:44:06.537652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义目标函数，Optuna会根据这个函数进行超参数搜索\n",
    "def objective(trial):\n",
    "    # 从Optuna的Trial对象中获取超参数\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)  # 学习率范围从 1e-5 到 1e-2\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, step=32)  # 批次大小范围 32 到 128，步长为32\n",
    "    num_channels = [\n",
    "        trial.suggest_int('n_channels_0', 16, 64, step=16),  # 第1层卷积的通道数\n",
    "        trial.suggest_int('n_channels_1', 32, 128, step=32),  # 第2层卷积的通道数\n",
    "        trial.suggest_int('n_channels_2', 64, 256, step=64),  # 第3层卷积的通道数\n",
    "    ]\n",
    "    \n",
    "    # 创建 TCNSequenceModel 实例\n",
    "    model = TCNSequenceModel(\n",
    "        n_epochs=50,  # 可以调整训练的epochs数量\n",
    "        lr=lr,\n",
    "        input_size=124,  # 假设输入特征的数量为124，可以根据实际情况调整\n",
    "        num_channels=num_channels,\n",
    "        dropout=0.2,\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    model.fit(train_dataset, valid_dataset)\n",
    "    \n",
    "    # 返回验证集损失，Optuna将最小化返回的值\n",
    "    return model.valid_losses[-1]  # 假设我们使用验证集损失作为评估指标\n",
    "\n",
    "# 创建Optuna的study对象，设置最小化目标\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# 运行优化过程\n",
    "study.optimize(objective, n_trials=10)  # 进行10次试验\n",
    "\n",
    "# 输出最佳超参数\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:02:14.863332Z",
     "iopub.status.busy": "2024-12-19T14:02:14.862973Z",
     "iopub.status.idle": "2024-12-19T14:02:14.874133Z",
     "shell.execute_reply": "2024-12-19T14:02:14.873399Z",
     "shell.execute_reply.started": "2024-12-19T14:02:14.863304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "param_path = \"/kaggle/working/master_15.pkl\"\n",
    "model.load_param(param_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:50:00.071779Z",
     "iopub.status.busy": "2024-12-19T14:50:00.070916Z",
     "iopub.status.idle": "2024-12-19T15:37:10.442683Z",
     "shell.execute_reply": "2024-12-19T15:37:10.441813Z",
     "shell.execute_reply.started": "2024-12-19T14:50:00.071749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 训练模型并记录损失\n",
    "train_losses, val_losses = model.fit(train_dataset, valid_dataset)\n",
    "\n",
    "# 绘制损失变化图像\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.707568Z",
     "iopub.status.idle": "2024-12-19T13:50:18.708001Z",
     "shell.execute_reply": "2024-12-19T13:50:18.707789Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.707768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # 初始化模型\n",
    "# model = TCNSequenceModel(\n",
    "#     n_epochs=10,\n",
    "#     lr=0.001,\n",
    "#     input_size=124,  # 特征维度（124-1，因为最后一维是标签）\n",
    "#     output_size=1,   # 预测维度\n",
    "#     num_channels=[32, 64, 128],  # TCN的通道数配置\n",
    "#     kernel_size=2,   # 卷积核大小\n",
    "#     dropout=0.2      # dropout率\n",
    "# )\n",
    "\n",
    "# # 训练模型\n",
    "# model.fit(train_dataset, valid_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model--github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.654311Z",
     "iopub.status.idle": "2024-12-19T13:50:18.654605Z",
     "shell.execute_reply": "2024-12-19T13:50:18.654477Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.654464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNGRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.655695Z",
     "iopub.status.idle": "2024-12-19T13:50:18.656007Z",
     "shell.execute_reply": "2024-12-19T13:50:18.655859Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.655847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "TCN_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 2*TCN_UNITS\n",
    "EPOCHS = 20\n",
    "MAX_LEN = 220\n",
    "NUM_MODELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.658023Z",
     "iopub.status.idle": "2024-12-19T13:50:18.658365Z",
     "shell.execute_reply": "2024-12-19T13:50:18.658236Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.658221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from https://github.com/philipperemy/keras-tcn\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import keras.layers\n",
    "from keras import optimizers\n",
    "# from keras.engine.topology import Layer\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Convolution1D, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def channel_normalization(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\" Normalize a layer to the maximum activation\n",
    "    This keeps a layers values between zero and one.\n",
    "    It helps with relu's unbounded activation\n",
    "    Args:\n",
    "        x: The layer to normalize\n",
    "    Returns:\n",
    "        A maximal normalized layer\n",
    "    \"\"\"\n",
    "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
    "    out = x / max_values\n",
    "    return out\n",
    "\n",
    "\n",
    "def wave_net_activation(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\"This method defines the activation used for WaveNet\n",
    "    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
    "    Args:\n",
    "        x: The layer we want to apply the activation to\n",
    "    Returns:\n",
    "        A new layer with the wavenet activation applied\n",
    "    \"\"\"\n",
    "    tanh_out = Activation('tanh')(x)\n",
    "    sigm_out = Activation('sigmoid')(x)\n",
    "    return keras.layers.multiply([tanh_out, sigm_out])\n",
    "\n",
    "\n",
    "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
    "    # type: (Layer, int, int, str, int, int, float, str) -> Tuple[Layer, Layer]\n",
    "    \"\"\"Defines the residual block for the WaveNet TCN\n",
    "    Args:\n",
    "        x: The previous layer in the model\n",
    "        s: The stack index i.e. which stack in the overall TCN\n",
    "        i: The dilation power of 2 we are using for this residual block\n",
    "        activation: The name of the type of activation to use\n",
    "        nb_filters: The number of convolutional filters to use in this block\n",
    "        kernel_size: The size of the convolutional kernel\n",
    "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "    Returns:\n",
    "        A tuple where the first element is the residual model layer, and the second\n",
    "        is the skip connection.\n",
    "    \"\"\"\n",
    "\n",
    "    original_x = x\n",
    "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding=padding,\n",
    "                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n",
    "    if activation == 'norm_relu':\n",
    "        x = Activation('relu')(conv)\n",
    "        x = Lambda(channel_normalization)(x)\n",
    "    elif activation == 'wavenet':\n",
    "        x = wave_net_activation(conv)\n",
    "    else:\n",
    "        x = Activation(activation)(conv)\n",
    "\n",
    "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
    "\n",
    "    # 1x1 conv.\n",
    "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
    "    res_x = keras.layers.add([original_x, x])\n",
    "    return res_x, x\n",
    "\n",
    "\n",
    "def process_dilations(dilations):\n",
    "    def is_power_of_two(num):\n",
    "        return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
    "        return new_dilations\n",
    "\n",
    "\n",
    "class TCN(Layer):\n",
    "    \"\"\"Creates a TCN layer.\n",
    "        Args:\n",
    "            input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n",
    "            nb_filters: The number of filters to use in the convolutional layers.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            activation: The activations to use (norm_relu, wavenet, relu...).\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            name: Name of the model. Useful when having multiple TCN.\n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=None,\n",
    "                 activation='norm_relu',\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=True,\n",
    "                 name='tcn'):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.activation = activation\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.padding = padding\n",
    "\n",
    "        # backwards incompatibility warning.\n",
    "        # o = tcn.TCN(i, return_sequences=False) =>\n",
    "        # o = tcn.TCN(return_sequences=False)(i)\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            print('An interface change occurred after the version 2.1.2.')\n",
    "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
    "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
    "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
    "            raise Exception()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        if self.dilations is None:\n",
    "            self.dilations = [1, 2, 4, 8, 16, 32]\n",
    "        x = inputs\n",
    "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
    "        skip_connections = []\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i in self.dilations:\n",
    "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
    "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
    "                skip_connections.append(skip_out)\n",
    "        if self.use_skip_connections:\n",
    "            x = keras.layers.add(skip_connections)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            output_slice_index = -1\n",
    "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.660800Z",
     "iopub.status.idle": "2024-12-19T13:50:18.661112Z",
     "shell.execute_reply": "2024-12-19T13:50:18.660961Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.660948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, add, Lambda, concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(K.reshape(y_true[:, 0], (-1, 1)), y_pred) * y_true[:, 1]\n",
    "\n",
    "\n",
    "# 构建模型函数\n",
    "def build_model(input_shape, loss_weight, tcn_units=64, dense_hidden_units=128):\n",
    "    \"\"\"\n",
    "    构建用于时序多特征数据的 TCN 模型。\n",
    "\n",
    "    参数:\n",
    "        input_shape: tuple, 输入数据的形状，例如 (time_steps, features)。\n",
    "        num_aux_targets: int, 辅助任务的目标数量。\n",
    "        loss_weight: float, 主任务损失的权重。\n",
    "        tcn_units: int, 每个 TCN 层的单元数量。\n",
    "        dense_hidden_units: int, 全连接层的隐藏单元数量。\n",
    "\n",
    "    返回:\n",
    "        model: keras Model 对象。\n",
    "    \"\"\"\n",
    "    # 输入层\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # 前向 TCN 层\n",
    "    x1 = TCN(tcn_units, return_sequences=True, dilations=[1, 2, 4, 8, 16], name='tcn1_forward')(inputs)\n",
    "\n",
    "    # 反向 TCN 层\n",
    "    x2 = Lambda(lambda z: K.reverse(z, axes=-2))(inputs) \n",
    "    x2 = TCN(tcn_units, return_sequences=True, dilations=[1, 2, 4, 8, 16], name='tcn1_backward')(x2)\n",
    "\n",
    "    # 合并前向和反向层\n",
    "    x = add([x1, x2])\n",
    "\n",
    "    # 第二层前向 TCN\n",
    "    x1 = TCN(tcn_units, return_sequences=True, dilations=[1, 2, 4, 8, 16], name='tcn2_forward')(x)\n",
    "\n",
    "    # 第二层反向 TCN\n",
    "    x2 = Lambda(lambda z: K.reverse(z, axes=-2))(x)  \n",
    "    x2 = TCN(tcn_units, return_sequences=True, dilations=[1, 2, 4, 8, 16], name='tcn2_backward')(x2)\n",
    "\n",
    "    # 合并前向和反向层\n",
    "    x = add([x1, x2])\n",
    "\n",
    "    # 全局池化\n",
    "    hidden = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])\n",
    "\n",
    "    # 全连接层 + 残差连接\n",
    "    hidden = add([hidden, Dense(dense_hidden_units, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(dense_hidden_units, activation='relu')(hidden)])\n",
    "\n",
    "    # 主任务输出\n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(hidden)\n",
    "\n",
    "    # 辅助任务输出\n",
    "    # aux_output = Dense(num_aux_targets, activation='sigmoid', name='aux_output')(hidden)\n",
    "\n",
    "    # 构建模型\n",
    "    model = Model(inputs=inputs, outputs=main_output)\n",
    "    model.compile(\n",
    "        loss=[custom_loss, 'binary_crossentropy'],\n",
    "        loss_weights=[loss_weight, 1.0],\n",
    "        optimizer=Adam()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.665360Z",
     "iopub.status.idle": "2024-12-19T13:50:18.665689Z",
     "shell.execute_reply": "2024-12-19T13:50:18.665519Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.665507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(train_dataset, valid_dataset, num_models, build_model_func, batch_size, epochs, loss_weight):\n",
    "    \"\"\"\n",
    "    训练模型，支持多模型集成和辅助任务。\n",
    "\n",
    "    参数：\n",
    "    - train_dataset: Tuple (X_train, [y_train_main, y_train_aux])，包含训练集的特征和标签。\n",
    "    - valid_dataset: Tuple (X_valid, [y_valid_main, y_valid_aux])，包含验证集的特征和标签。\n",
    "    - num_models: 训练模型的数量，用于模型集成。\n",
    "    - build_model_func: 构建模型的函数。\n",
    "    - batch_size: 批次大小。\n",
    "    - epochs: 每个模型的训练轮次。\n",
    "    - loss_weight: 主任务的损失权重。\n",
    "\n",
    "    返回：\n",
    "    - predictions: 对验证集的最终预测结果。\n",
    "    \"\"\"\n",
    "    checkpoint_predictions = []\n",
    "    checkpoint_val_preds = []\n",
    "    weights = []\n",
    "\n",
    "    X_train, [y_train_main, y_train_aux] = train_dataset\n",
    "    X_valid, [y_valid_main, y_valid_aux] = valid_dataset\n",
    "\n",
    "    for model_idx in range(num_models):\n",
    "        # 构建模型\n",
    "        model = build_model_func(X_train.shape[1:], y_train_aux.shape[-1], loss_weight)\n",
    "        for global_epoch in range(epochs):\n",
    "            # 提早停止和学习率调度器\n",
    "            es = EarlyStopping(patience=1, verbose=True)\n",
    "            lr_scheduler = LearningRateScheduler(lambda epoch: 2e-3 * (0.6 ** global_epoch))\n",
    "\n",
    "            # 模型训练\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                [y_train_main, y_train_aux],\n",
    "                validation_data=(X_valid, [y_valid_main, y_valid_aux]),\n",
    "                batch_size=batch_size,\n",
    "                epochs=1,\n",
    "                verbose=1,\n",
    "                callbacks=[lr_scheduler, es]\n",
    "            )\n",
    "\n",
    "            # 验证集预测\n",
    "            checkpoint_val_preds.append(model.predict(X_valid, batch_size=batch_size)[0].flatten())\n",
    "\n",
    "            # 保存权重\n",
    "            weights.append(2 ** global_epoch)\n",
    "\n",
    "        # 清理内存\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "    # 计算验证集的加权平均预测\n",
    "    predictions = np.average(checkpoint_val_preds, weights=weights, axis=0)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.669236Z",
     "iopub.status.idle": "2024-12-19T13:50:18.669526Z",
     "shell.execute_reply": "2024-12-19T13:50:18.669405Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.669391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.00001):\n",
    "        self.best_model = None\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        \n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def early_stop(self, validation_loss, model):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            print(f\"New best loss: {validation_loss:>4f}\")\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = model\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.673165Z",
     "iopub.status.idle": "2024-12-19T13:50:18.673568Z",
     "shell.execute_reply": "2024-12-19T13:50:18.673380Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.673358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.train_loss, \"g:\", label=\"Train Loss\")\n",
    "plt.plot(history.epoch, history.test_loss, \"r--\", label=\"Test Loss\")\n",
    "ax2 = plt.twinx()\n",
    "ax2.plot(history.epoch, history.lr, c=\"yellow\", label=\"Learning Rate\")\n",
    "ax2.set_ylim(0, ax2.get_ylim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.675090Z",
     "iopub.status.idle": "2024-12-19T13:50:18.675513Z",
     "shell.execute_reply": "2024-12-19T13:50:18.675322Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.675302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = NeuralNetwork()\n",
    "d = next(iter(test_dataloader))[0]\n",
    "pred = predict(d, model)\n",
    "res = pd.DataFrame({\n",
    "    \"target\":next(iter(test_dataloader))[1].flatten().cpu(), \n",
    "    \"pred\":pred\n",
    "})\n",
    "res[\"err\"] = np.abs(res[\"target\"] - res[\"pred\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.676667Z",
     "iopub.status.idle": "2024-12-19T13:50:18.677087Z",
     "shell.execute_reply": "2024-12-19T13:50:18.676884Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.676865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot([res.target.min(), res.target.max()], [res.target.min(), res.target.max()], color=\"gray\")\n",
    "plt.scatter(res.target, res.pred, marker=\"x\")\n",
    "plt.ylim(res.pred.min()*1.5,res.pred.max()*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.678188Z",
     "iopub.status.idle": "2024-12-19T13:50:18.678578Z",
     "shell.execute_reply": "2024-12-19T13:50:18.678396Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.678377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.hist(res[\"pred\"], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.680377Z",
     "iopub.status.idle": "2024-12-19T13:50:18.680775Z",
     "shell.execute_reply": "2024-12-19T13:50:18.680587Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.680568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.681716Z",
     "iopub.status.idle": "2024-12-19T13:50:18.682110Z",
     "shell.execute_reply": "2024-12-19T13:50:18.681930Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.681911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full_submission_data = df_raw.iloc[0:0,:]\n",
    "full_prediction_data = pd.DataFrame([], columns=[\"\"])\n",
    "full_submission_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.683505Z",
     "iopub.status.idle": "2024-12-19T13:50:18.683759Z",
     "shell.execute_reply": "2024-12-19T13:50:18.683647Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.683635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for (raw_df_test, _, sample_prediction) in iter_test: \n",
    "    \n",
    "    full_submission_data = pd.concat((full_submission_data,raw_df_test), axis=0)\n",
    "\n",
    "    df_test = add_info_columns(full_submission_data)\n",
    "    x_test = df_test.loc[:,x_cols].iloc[-200:,:]\n",
    "    x_test_normalized = normalize_features(x_test).fillna(0.0).values\n",
    "    \n",
    "    pred = predict(torch.Tensor(x_test_normalized).to(device), model)\n",
    "    \n",
    "    sample_prediction['target'] = pred\n",
    "    env.predict(sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.684671Z",
     "iopub.status.idle": "2024-12-19T13:50:18.684925Z",
     "shell.execute_reply": "2024-12-19T13:50:18.684814Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.684802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T13:50:18.686472Z",
     "iopub.status.idle": "2024-12-19T13:50:18.686766Z",
     "shell.execute_reply": "2024-12-19T13:50:18.686641Z",
     "shell.execute_reply.started": "2024-12-19T13:50:18.686627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sample_prediction[\"target\"], bins=20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 6079876,
     "sourceId": 9928768,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "JaneStreet_py310",
   "language": "python",
   "name": "py310js"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
