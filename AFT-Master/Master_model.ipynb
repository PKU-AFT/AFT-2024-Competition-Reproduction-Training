{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要先把润楠的数据设置成多级表，代码类似：multiindex_df = flat_df.set_index(['time_id', 'stock_id'])\n",
    "\n",
    "# multiindex_df -> TSDataSampler -> DailyBatchSamplerRandom -> DataLoader(TSDataSampler, DailyBatchSamplerRandom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset && DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Union, List\n",
    "\n",
    "import bisect\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_sort_index(df: pd.DataFrame, axis=0) -> pd.DataFrame:\n",
    "    idx = df.index if axis == 0 else df.columns\n",
    "    if (\n",
    "        not idx.is_monotonic_increasing\n",
    "        and isinstance(idx, pd.MultiIndex)\n",
    "        and not idx.is_lexsorted()\n",
    "    ):  \n",
    "        return df.sort_index(axis=axis)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def np_ffill(arr: np.array):\n",
    "    mask = np.isnan(arr.astype(float))  # np.isnan only works on np.float\n",
    "    # get fill index\n",
    "    idx = np.where(~mask, np.arange(mask.shape[0]), 0)\n",
    "    np.maximum.accumulate(idx, out=idx)\n",
    "    return arr[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSDataSampler (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataSampler:\n",
    "    \"\"\"\n",
    "    (T)ime-(S)eries DataSampler\n",
    "    This is the result of TSDatasetH\n",
    "\n",
    "    It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series\n",
    "    dataset based on tabular data.\n",
    "    - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future\n",
    "      data.\n",
    "\n",
    "    If user have further requirements for processing data, user could process them based on `TSDataSampler` or create\n",
    "    more powerful subclasses.\n",
    "\n",
    "    Known Issues:\n",
    "    - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result\n",
    "      in a different data type\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, data: pd.DataFrame, start, end, step_len: int, fillna_type: str = \"none\", dtype=None, flt_data=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a dataset which looks like torch.data.utils.Dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            The raw tabular data\n",
    "        start :\n",
    "            The indexable start time\n",
    "        end :\n",
    "            The indexable end time\n",
    "        step_len : int\n",
    "            The length of the time-series step\n",
    "        fillna_type : int\n",
    "            How will qlib handle the sample if there is on sample in a specific date.\n",
    "            none:\n",
    "                fill with np.nan\n",
    "            ffill:\n",
    "                ffill with previous sample\n",
    "            ffill+bfill:\n",
    "                ffill with previous samples first and fill with later samples second\n",
    "        flt_data : pd.Series\n",
    "            a column of data(True or False) to filter data.\n",
    "            None:\n",
    "                kepp all data\n",
    "\n",
    "        \"\"\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.step_len = step_len\n",
    "        self.fillna_type = fillna_type\n",
    "        self.data = lazy_sort_index(data)\n",
    "\n",
    "        kwargs = {\"object\": self.data}\n",
    "        if dtype is not None:\n",
    "            kwargs[\"dtype\"] = dtype\n",
    "\n",
    "        self.data_arr = np.array(**kwargs)  # Get index from numpy.array will much faster than DataFrame.values!\n",
    "        # NOTE:\n",
    "        # - append last line with full NaN for better performance in `__getitem__`\n",
    "        # - Keep the same dtype will result in a better performance\n",
    "        self.data_arr = np.append(\n",
    "            self.data_arr, np.full((1, self.data_arr.shape[1]), np.nan, dtype=self.data_arr.dtype), axis=0\n",
    "        )\n",
    "        self.nan_idx = -1  # The last line is all NaN\n",
    "\n",
    "        # the data type will be changed\n",
    "        # The index of usable data is between start_idx and end_idx\n",
    "        self.idx_df, self.idx_map = self.build_index(self.data)\n",
    "        self.data_index = deepcopy(self.data.index)\n",
    "\n",
    "        if flt_data is not None:\n",
    "            if isinstance(flt_data, pd.DataFrame):\n",
    "                assert len(flt_data.columns) == 1\n",
    "                flt_data = flt_data.iloc[:, 0]\n",
    "            # NOTE: bool(np.nan) is True !!!!!!!!\n",
    "            # make sure reindex comes first. Otherwise extra NaN may appear.\n",
    "            flt_data = flt_data.reindex(self.data_index).fillna(False).astype(np.bool)\n",
    "            self.flt_data = flt_data.values\n",
    "            self.idx_map = self.flt_idx_map(self.flt_data, self.idx_map)\n",
    "            self.data_index = self.data_index[np.where(self.flt_data)[0]]\n",
    "        self.idx_map = self.idx_map2arr(self.idx_map)\n",
    "\n",
    "        self.start_idx, self.end_idx = self.data_index.slice_locs(\n",
    "            start=start, end=end)\n",
    "        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)  # for better performance\n",
    "\n",
    "        del self.data  # save memory\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_map2arr(idx_map):\n",
    "        # pytorch data sampler will have better memory control without large dict or list\n",
    "        # - https://github.com/pytorch/pytorch/issues/13243\n",
    "        # - https://github.com/airctic/icevision/issues/613\n",
    "        # So we convert the dict into int array.\n",
    "        # The arr_map is expected to behave the same as idx_map\n",
    "\n",
    "        dtype = np.int64\n",
    "        # set a index out of bound to indicate the none existing\n",
    "        no_existing_idx = (np.iinfo(dtype).max, np.iinfo(dtype).max)\n",
    "\n",
    "        max_idx = max(idx_map.keys())\n",
    "        arr_map = []\n",
    "        for i in range(max_idx + 1):\n",
    "            arr_map.append(idx_map.get(i, no_existing_idx))\n",
    "        arr_map = np.array(arr_map, dtype=dtype)\n",
    "        return arr_map\n",
    "\n",
    "    @staticmethod\n",
    "    def flt_idx_map(flt_data, idx_map):\n",
    "        idx = 0\n",
    "        new_idx_map = {}\n",
    "        for i, exist in enumerate(flt_data):\n",
    "            if exist:\n",
    "                new_idx_map[idx] = idx_map[i]\n",
    "                idx += 1\n",
    "        return new_idx_map\n",
    "\n",
    "    def get_index(self):\n",
    "        \"\"\"\n",
    "        Get the pandas index of the data, it will be useful in following scenarios\n",
    "        - Special sampler will be used (e.g. user want to sample day by day)\n",
    "        \"\"\"\n",
    "        return self.data_index[self.start_idx : self.end_idx]\n",
    "\n",
    "    def config(self, **kwargs):\n",
    "        # Config the attributes\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_index(data: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "        \"\"\"\n",
    "        The relation of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            The dataframe with <datetime, DataFrame>\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[pd.DataFrame, dict]:\n",
    "            1) the first element:  reshape the original index into a <datetime(row), instrument(column)> 2D dataframe\n",
    "                instrument SH600000 SH600004 SH600006 SH600007 SH600008 SH600009  ...\n",
    "                datetime\n",
    "                2021-01-11        0        1        2        3        4        5  ...\n",
    "                2021-01-12     4146     4147     4148     4149     4150     4151  ...\n",
    "                2021-01-13     8293     8294     8295     8296     8297     8298  ...\n",
    "                2021-01-14    12441    12442    12443    12444    12445    12446  ...\n",
    "            2) the second element:  {<original index>: <row, col>}\n",
    "        \"\"\"\n",
    "        # object incase of pandas converting int to float\n",
    "        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=object)\n",
    "        idx_df = lazy_sort_index(idx_df.unstack())\n",
    "        # NOTE: the correctness of `__getitem__` depends on columns sorted here\n",
    "        idx_df = lazy_sort_index(idx_df, axis=1)\n",
    "\n",
    "        idx_map = {}\n",
    "        for i, (_, row) in enumerate(idx_df.iterrows()):\n",
    "            for j, real_idx in enumerate(row):\n",
    "                if not np.isnan(real_idx):\n",
    "                    idx_map[real_idx] = (i, j)\n",
    "        return idx_df, idx_map\n",
    "\n",
    "    @property\n",
    "    def empty(self):\n",
    "        return len(self) == 0\n",
    "\n",
    "    def _get_indices(self, row: int, col: int) -> np.array:\n",
    "        \"\"\"\n",
    "        get series indices of self.data_arr from the row, col indices of self.idx_df\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row : int\n",
    "            the row in self.idx_df\n",
    "        col : int\n",
    "            the col in self.idx_df\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            The indices of data of the data\n",
    "        \"\"\"\n",
    "        indices = self.idx_arr[max(row - self.step_len + 1, 0) : row + 1, col]\n",
    "\n",
    "        if len(indices) < self.step_len:\n",
    "            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])\n",
    "\n",
    "        if self.fillna_type == \"ffill\":\n",
    "            indices = np_ffill(indices)\n",
    "        elif self.fillna_type == \"ffill+bfill\":\n",
    "            indices = np_ffill(np_ffill(indices)[::-1])[::-1]\n",
    "        else:\n",
    "            assert self.fillna_type == \"none\"\n",
    "        return indices\n",
    "\n",
    "    def _get_row_col(self, idx) -> Tuple[int]:\n",
    "        \"\"\"\n",
    "        get the col index and row index of a given sample index in self.idx_df\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx :\n",
    "            the input of  `__getitem__`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[int]:\n",
    "            the row and col index\n",
    "        \"\"\"\n",
    "        # The the right row number `i` and col number `j` in idx_df\n",
    "        if isinstance(idx, (int, np.integer)):\n",
    "            real_idx = self.start_idx + idx\n",
    "            if self.start_idx <= real_idx < self.end_idx:\n",
    "                i, j = self.idx_map[real_idx]  # TODO: The performance of this line is not good\n",
    "            else:\n",
    "                raise KeyError(f\"{real_idx} is out of [{self.start_idx}, {self.end_idx})\")\n",
    "        elif isinstance(idx, tuple):\n",
    "            # <TSDataSampler object>[\"datetime\", \"instruments\"]\n",
    "            date, inst = idx\n",
    "            date = pd.Timestamp(date)\n",
    "            i = bisect.bisect_right(self.idx_df.index, date) - 1\n",
    "            # NOTE: This relies on the idx_df columns sorted in `__init__`\n",
    "            j = bisect.bisect_left(self.idx_df.columns, inst)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"This type of input is not supported\")\n",
    "        return i, j\n",
    "\n",
    "    def __getitem__(self, idx: Union[int, Tuple[object, str], List[int]]):\n",
    "        \"\"\"\n",
    "        # We have two method to get the time-series of a sample\n",
    "        tsds is a instance of TSDataSampler\n",
    "\n",
    "        # 1) sample by int index directly\n",
    "        tsds[len(tsds) - 1]\n",
    "\n",
    "        # 2) sample by <datetime,instrument> index\n",
    "        tsds['2016-12-31', \"SZ300315\"]\n",
    "\n",
    "        # The return value will be similar to the data retrieved by following code\n",
    "        df.loc(axis=0)['2015-01-01':'2016-12-31', \"SZ300315\"].iloc[-30:]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : Union[int, Tuple[object, str]]\n",
    "        \"\"\"\n",
    "        # Multi-index type\n",
    "        mtit = (list, np.ndarray)\n",
    "        if isinstance(idx, mtit):\n",
    "            indices = [self._get_indices(*self._get_row_col(i)) for i in idx]\n",
    "            indices = np.concatenate(indices)\n",
    "        else:\n",
    "            indices = self._get_indices(*self._get_row_col(idx))\n",
    "\n",
    "        # 1) for better performance, use the last nan line for padding the lost date\n",
    "        # 2) In case of precision problems. We use np.float64. # TODO: I'm not sure if whether np.float64 will result in\n",
    "        # precision problems. It will not cause any problems in my tests at least\n",
    "        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)\n",
    "\n",
    "        data = self.data_arr[indices]\n",
    "        if isinstance(idx, mtit):\n",
    "            # if we get multiple indexes, addition dimension should be added.\n",
    "            # <sample_idx, step_idx, feature_idx>\n",
    "            data = data.reshape(-1, self.step_len, *data.shape[1:])\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DailyBatchSamplerRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyBatchSamplerRandom(Sampler):\n",
    "    def __init__(self, data_source, shuffle=False):\n",
    "        self.data_source = data_source\n",
    "        self.shuffle = shuffle\n",
    "        # calculate number of samples in each batch\n",
    "        self.daily_count = pd.Series(index=self.data_source.get_index(), dtype=pd.Float32Dtype).groupby(\"time_id\").size().values\n",
    "        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch\n",
    "        self.daily_index[0] = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            index = np.arange(len(self.daily_count))\n",
    "            np.random.shuffle(index)\n",
    "            for i in index:\n",
    "                yield np.arange(self.daily_index[i], self.daily_index[i] + self.daily_count[i])\n",
    "        else:\n",
    "            for idx, count in zip(self.daily_index, self.daily_count):\n",
    "                yield np.arange(idx, idx + count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_for_model.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data.drop(['row_id', 'seconds_in_bucket', 'date_id', '__index_level_0__'], axis=1)\n",
    "\n",
    "cols = list(data.columns)\n",
    "cols.remove('target')\n",
    "cols.append('target')\n",
    "\n",
    "data = data[cols]\n",
    "\n",
    "# to float32\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "print(data.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time_id = data.index.get_level_values('time_id').max()\n",
    "\n",
    "print(\"max_time_id\", max_time_id)\n",
    "\n",
    "# split 4:1\n",
    "split_time_id = int(max_time_id * 0.8)\n",
    "\n",
    "train_data = data.query('time_id <= @split_time_id').copy()\n",
    "valid_data = data.query('time_id > @split_time_id').copy()\n",
    "\n",
    "# del data\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TSDataSampler(data=train_data, start=0, end=split_time_id, step_len=10, fillna_type='ffill+bfill',)\n",
    "valid_dataset = TSDataSampler(data=valid_data, start=split_time_id+1, end=max_time_id, step_len=10, fillna_type='ffill+bfill',)\n",
    "\n",
    "train_sampler = DailyBatchSamplerRandom(train_dataset, shuffle=False)\n",
    "valid_sampler = DailyBatchSamplerRandom(valid_dataset, shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    data = torch.squeeze(data, dim=0)\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequenceModel (Base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel():\n",
    "    def __init__(self, n_epochs, lr, GPU=None, seed=None, train_stop_loss_thred=None, save_path = 'model/', save_prefix= ''):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.device = torch.device(\"mps:0\")\n",
    "        self.seed = seed\n",
    "        self.train_stop_loss_thred = train_stop_loss_thred\n",
    "\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "        self.fitted = False\n",
    "        self.model = None\n",
    "        self.train_optimizer = None\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.save_prefix = save_prefix\n",
    "\n",
    "\n",
    "    def init_model(self):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"model has not been initialized\")\n",
    "\n",
    "        self.train_optimizer = optim.Adam(self.model.parameters(), self.lr)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def loss_fn(self, pred, label):\n",
    "        mask = ~torch.isnan(label)\n",
    "        loss = torch.abs(pred[mask] - label[mask])\n",
    "        return torch.mean(loss)\n",
    "\n",
    "    def train_epoch(self, data_loader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        for data in data_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            '''\n",
    "            data.shape: (N, T, F)\n",
    "            N - number of stocks\n",
    "            T - length of lookback_window, 10\n",
    "            F - length of PCA feature, 124           \n",
    "            '''\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1].to(self.device)\n",
    "\n",
    "            pred = self.model(feature.float())\n",
    "            loss = self.loss_fn(pred, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.train_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.model.parameters(), 1.0) \n",
    "            self.train_optimizer.step()\n",
    "\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    def test_epoch(self, data_loader):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "\n",
    "        for data in data_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1].to(self.device)\n",
    "            pred = self.model(feature.float())\n",
    "            loss = self.loss_fn(pred, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    def _init_data_loader(self, data, shuffle=True, drop_last=True):\n",
    "        sampler = DailyBatchSamplerRandom(data, shuffle)\n",
    "        data_loader = DataLoader(data, sampler=sampler, drop_last=drop_last)\n",
    "        return data_loader\n",
    "\n",
    "    def load_param(self, param_path):\n",
    "        self.model.load_state_dict(torch.load(param_path, map_location=self.device))\n",
    "        self.fitted = True\n",
    "\n",
    "    def fit(self, dl_train, dl_valid):\n",
    "        train_loader = self._init_data_loader(dl_train, shuffle=True, drop_last=True)\n",
    "        valid_loader = self._init_data_loader(dl_valid, shuffle=False, drop_last=True)\n",
    "\n",
    "        self.fitted = True\n",
    "        best_param = None\n",
    "        for step in range(self.n_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.test_epoch(valid_loader)\n",
    "\n",
    "            print(\"Epoch %d, train_loss %.6f, valid_loss %.6f \" % (step, train_loss, val_loss))\n",
    "            best_param = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "            # if train_loss <= self.train_stop_loss_thred:\n",
    "            #     break\n",
    "            torch.save(best_param, f'{self.save_path}{self.save_prefix}master_{step}.pkl')\n",
    "\n",
    "    def predict(self, dl_test):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"model is not fitted yet!\")\n",
    "\n",
    "        test_loader = self._init_data_loader(dl_test, shuffle=False, drop_last=False)\n",
    "\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        self.model.eval()\n",
    "        for data in test_loader:\n",
    "            data = torch.squeeze(data, dim=0)\n",
    "            feature = data[:, :, 0:-1].to(self.device)\n",
    "            label = data[:, -1, -1]\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(feature.float()).detach().cpu().numpy()\n",
    "            preds.append(pred.ravel())\n",
    "            labels.append(label.numpy().ravel())\n",
    "\n",
    "        predictions = pd.Series(np.concatenate(preds), index=dl_test.get_index())\n",
    "\n",
    "\n",
    "        return predictions, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.shape[1], :]\n",
    "\n",
    "\n",
    "class SAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.temperature = math.sqrt(self.d_model/nhead)\n",
    "\n",
    "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        attn_dropout_layer = []\n",
    "        for i in range(nhead):\n",
    "            attn_dropout_layer.append(Dropout(p=dropout))\n",
    "        self.attn_dropout = nn.ModuleList(attn_dropout_layer)\n",
    "\n",
    "        # input LayerNorm\n",
    "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "        # FFN layerNorm\n",
    "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            Dropout(p=dropout),\n",
    "            Linear(d_model, d_model),\n",
    "            Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        q = self.qtrans(x).transpose(0,1) # [N, T, D] --> [T, N, D]\n",
    "        k = self.ktrans(x).transpose(0,1)\n",
    "        v = self.vtrans(x).transpose(0,1)\n",
    "\n",
    "        dim = int(self.d_model/self.nhead)\n",
    "        att_output = []\n",
    "        for i in range(self.nhead):\n",
    "            if i==self.nhead-1:\n",
    "                qh = q[:, :, i * dim:]\n",
    "                kh = k[:, :, i * dim:]\n",
    "                vh = v[:, :, i * dim:]\n",
    "            else:\n",
    "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
    "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
    "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
    "\n",
    "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)) / self.temperature, dim=-1)\n",
    "            if self.attn_dropout:\n",
    "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh)\n",
    "            att_output.append(torch.matmul(atten_ave_matrixh, vh).transpose(0, 1)) # [T, N, D] --> [N, T, D]\n",
    "        att_output = torch.concat(att_output, dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        xt = x + att_output\n",
    "        xt = self.norm2(xt)\n",
    "        att_output = xt + self.ffn(xt)\n",
    "\n",
    "        return att_output\n",
    "\n",
    "\n",
    "class TAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.attn_dropout = []\n",
    "        if dropout > 0:\n",
    "            for i in range(nhead):\n",
    "                self.attn_dropout.append(Dropout(p=dropout))\n",
    "            self.attn_dropout = nn.ModuleList(self.attn_dropout)\n",
    "\n",
    "        # input LayerNorm\n",
    "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
    "        # FFN layerNorm\n",
    "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            Dropout(p=dropout),\n",
    "            Linear(d_model, d_model),\n",
    "            Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        q = self.qtrans(x)\n",
    "        k = self.ktrans(x)\n",
    "        v = self.vtrans(x)\n",
    "\n",
    "        dim = int(self.d_model / self.nhead)\n",
    "        att_output = []\n",
    "        for i in range(self.nhead):\n",
    "            if i==self.nhead-1:\n",
    "                qh = q[:, :, i * dim:]\n",
    "                kh = k[:, :, i * dim:]\n",
    "                vh = v[:, :, i * dim:]\n",
    "            else:\n",
    "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
    "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
    "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
    "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)), dim=-1)\n",
    "            if self.attn_dropout:\n",
    "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh)\n",
    "            att_output.append(torch.matmul(atten_ave_matrixh, vh))\n",
    "        att_output = torch.concat(att_output, dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        xt = x + att_output\n",
    "        xt = self.norm2(xt)\n",
    "        att_output = xt + self.ffn(xt)\n",
    "\n",
    "        return att_output\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.trans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.trans(z) # [N, T, D]\n",
    "        query = h[:, -1, :].unsqueeze(-1)\n",
    "        lam = torch.matmul(h, query).squeeze(-1)  # [N, T, D] --> [N, T]\n",
    "        lam = torch.softmax(lam, dim=1).unsqueeze(1)\n",
    "        output = torch.matmul(lam, z).squeeze(1)  # [N, 1, T], [N, T, D] --> [N, 1, D]\n",
    "        return output\n",
    "\n",
    "\n",
    "class MASTER(nn.Module):\n",
    "    def __init__(self, d_feat=124, d_model=256, t_nhead=4, s_nhead=2, T_dropout_rate=0.5, S_dropout_rate=0.5):\n",
    "        super(MASTER, self).__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            # feature layer\n",
    "            nn.Linear(d_feat, d_model),\n",
    "            PositionalEncoding(d_model),\n",
    "            # intra-stock aggregation\n",
    "            TAttention(d_model=d_model, nhead=t_nhead, dropout=T_dropout_rate),\n",
    "            # inter-stock aggregation\n",
    "            SAttention(d_model=d_model, nhead=s_nhead, dropout=S_dropout_rate),\n",
    "            TemporalAttention(d_model=d_model),\n",
    "            # decoder\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x).squeeze(-1)\n",
    "        output = output - torch.mean(output, dim=0, keepdim=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MASTERModel(SequenceModel):\n",
    "    def __init__(\n",
    "            self, d_feat: int = 20, d_model: int = 64, t_nhead: int = 4, s_nhead: int = 2,\n",
    "            T_dropout_rate=0.5, S_dropout_rate=0.5, **kwargs,\n",
    "    ):\n",
    "        super(MASTERModel, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.d_feat = d_feat\n",
    "\n",
    "        self.T_dropout_rate = T_dropout_rate\n",
    "        self.S_dropout_rate = S_dropout_rate\n",
    "        self.t_nhead = t_nhead\n",
    "        self.s_nhead = s_nhead\n",
    "\n",
    "        self.init_model()\n",
    "\n",
    "    def init_model(self):\n",
    "        self.model = MASTER(d_feat=self.d_feat, d_model=self.d_model, t_nhead=self.t_nhead, s_nhead=self.s_nhead,\n",
    "                                   T_dropout_rate=self.T_dropout_rate, S_dropout_rate=self.S_dropout_rate)\n",
    "        super(MASTERModel, self).init_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = 'optiver'\n",
    "\n",
    "d_feat = 124\n",
    "d_model = 256\n",
    "t_nhead = 4\n",
    "s_nhead = 2\n",
    "dropout = 0.5\n",
    "\n",
    "n_epoch = 40\n",
    "lr = 8e-6\n",
    "GPU = 0\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MASTERModel(\n",
    "    d_feat=d_feat, d_model=d_model, t_nhead=t_nhead, s_nhead=s_nhead, T_dropout_rate=dropout, S_dropout_rate=dropout,\n",
    "    n_epochs=n_epoch, lr=lr, GPU=GPU, seed=seed,\n",
    "    save_path='model/', save_prefix=universe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(train_dataset, valid_dataset)\n",
    "print(\"Model Trained.\")\n",
    "# Test\n",
    "predictions, metrics = model.predict(dl_test)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load and Test\n",
    "# param_path = f'./model/{universe}master_19.pkl'\n",
    "# model.load_param(param_path)\n",
    "# predictions, labels = model.predict(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat labels [[], [], []] -> []\n",
    "labels = [item for sublist in labels for item in sublist]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nan\n",
    "np.sum(np.isnan(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions.values\n",
    "pred = np.array(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(labels)\n",
    "mae = np.mean(np.abs(pred[mask] - labels[mask]))\n",
    "mae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crn",
   "language": "python",
   "name": "py39crn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
